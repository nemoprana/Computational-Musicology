---
title: "Exploring SAAN"
subtitle: "Comparing the self-image to the raw data"
author: "Nemo Woudenberg"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
---

```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(spotifyr)
library(plotly)
library(compmus)
library(tidymodels)
library(ggdendro)
library(heatmaply)
```

An introduction to the data
====================================
## Empty {data-width="100"}

## Column {data-width="800"}

### Introduction

#### ALL NEW INFORMATION IS ON THE [Heatmap + dendogram] PAGE!!

Here I will be comparing some interesting data. On the one hand we have one side of the data:

 - My dad has 19 total songs on spotify: 11 songs played with the band 'Mad Stream' and 8 more songs with the band 'Sleepers Awake At Night'. These two bands consist of mostly the same musicians and have (in my opinion) a very similar sound. 

I want to use the songs of these two bands to have some fun and make an interesting comparison with another part of the data:

 - The band 'Sleepers Awake At Night' (from now on I will refer to them as SAAN, also when SAANs music is mentioned I actually mean the music of SAAN + Mad Stream ) has a public playlist on their spotify profile called 'radio SAAN'. The playlist consists of 94 songs and features some SAAN and Mad Stream songs, but most of the music is from different bands. 

The purpose of this playlist is to give the listener some music that is kind of like the music of SAAN, or maybe has been an inspiration for the band members. My research will therefore try to look at the next question:

Does SAAN sound like they think they do?

By comparing the playlist with their own songs we might get some insight into the kinds of things the members of SAAN listen to in the music they appreciate. For example: we might find that most of the songs in the radio SAAN playlist have a high score on valence. If the songs made by SAAN also tend to have a high valence, we could conclude that they appreciate a good mood! In [The first data] we will look at some introductory visualizations exploring some characteristics of SAANs music.

## Empty {data-width="100"}

The first data {data-navmenu="First Datas"}
====================================

## Graph {data-width="700"}

### First chart

```{r}
SAAN <- get_playlist_audio_features("", "2eBYBIoOoB3AIKyGvbVo3R")

gg_temp_val <- ggplot(SAAN, aes(tempo, valence, color = track.album.name)) +
  geom_point() +
  theme_minimal() +
  labs(x = "Tempo", y = "Valence", color = "Album") 

ggplotly(gg_temp_val)
```

## Text {data-width="300"}

### A small conclusion

The graph here on the left show us a quick overview of the valence and tempo of all the SAAN songs. We notice something interesting for the valence values: most of the songs score above 0.5, but 4 of them eek out below from under 0.3! And the low-valence songs are from different albums, so its not like they had an emo phase... We will have to look further into this!

The tempo values are pretty much smeared out, even though 125 bpm is preferably avoided. 

Spectogram {data-navmenu="First Datas" data-orientation=rows}
====================================
## Text {data-height="300"}

### Time for a spectogram!

Below here are two visualizations for the song 'Just You and Me'. In [The first data] this was the song with the lowest valence and an high tempo compared to the other low-valence songs (on the bottom-right).

Interesting to note in the timbre is that c05 actually roughly indicates where the voice enters! Bright yellow means there is voice present and the darker regions show absence of voice. The pitch on the other hand show us a very different distribution of colors. This gives us a good clue as to where the different sections in the song are. The part from ~60s until ~100s show us an interesting thing: the yellow parts on C# and F# are actually quite clearly what the guitar is playing. Because there is not too much harmonic information the lines are very distinct!

## Graph {data-height="700"}

### Timbre

```{r}
yandm <-
  get_tidy_audio_analysis("1e7FgxUJ8WAdbVMrpaFpsE") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "max", norm = "chebyshev"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "max", norm = "chebyshev"              # Change summary & norm.
      )
  )

yandm|>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()
```

### Pitch

```{r}
you_and_me <-
  get_tidy_audio_analysis("1e7FgxUJ8WAdbVMrpaFpsE") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

you_and_me |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

The chordogram! {data-navmenu="First Datas"}
====================================

## Graph {data-width="500"}

### Chordogram

```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

tpf <-
  get_tidy_audio_analysis("1yVIMaoLkTQkqhghCiTrNy") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"
      )
  )

tpf |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

## Text {data-width="500"}

### What to see

Here on the left is a chordogram for the song called 20point50! This one is very interesting and I will tell you why. This song (and most music in the corpus) is quite vampy (which means that the song just hangs around one chord / harmony instead of going all different hamonic places). This means that the keygram I made for this song actually shows very little matter of interest (check it out on the [Keygram] page!). When I tried making a chordogram, initially it didn't really work out until I sorted on sections. Now here you can really see what is going on! 

Most of the song is based around a C7 vamp (actually more based on a riff but that is not really important), however there are the little sections in between where the band basically just straight up sings a big fat F7 chord. The chordogram really shows this and it turned out great!

Keygram {data-navmenu="First Datas"}
====================================

## Graph {data-width="700"}

### Chordogram

```{r}
tpf |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

Novelty Function {data-navmenu="First Datas"}
====================================
## Text {data-width="300"}

### What is going on?
Here is a novelty graph for the song '20point50'. The song actually starts around 11 seconds, the beginning is filled with "electric secrets" (their words, not mine!). This is actually very visible in the novelty graph, where at around 11 seconds there is a huge spike in novelty! After this The novelty stays pretty much constant, and in the next page ([Tempogram])we will see what the bpm is for this song.


## Graph {data-width="700"}

### Graph
```{r}
twpf <-
  get_tidy_audio_analysis("1yVIMaoLkTQkqhghCiTrNy") |>
  select(segments) |>
  unnest(segments)

twpf |>
  mutate(loudness_max_time = start + loudness_max_time) |>
  arrange(loudness_max_time) |>
  mutate(delta_loudness = loudness_max - lag(loudness_max)) |>
  ggplot(aes(x = loudness_max_time, y = pmax(0, delta_loudness))) +
  geom_line() +
  xlim(0, 30) +
  theme_minimal() +
  labs(x = "Time (s)", y = "Novelty")
```

Tempogram {data-navmenu="First Datas"}
====================================

## Graph {data-width="700"}

### Graph
```{r}
tempo <- get_tidy_audio_analysis("1yVIMaoLkTQkqhghCiTrNy")

tempo |>
  tempogram(window_size = 8, hop_size = 3, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

## Text {data-width="300"}

### BPM
As seen before in the [Novelty Function], the tempogram also shows that the first few seconds of the song are different from the rest. After the intro the actual song starts and the tempo stays constant. All the music repetoire of SAAN is based around guitar looping, so it is no surprise to see that the tempo stays so constant, as the underlying music is quite literally repeating (they do switch sections where the tempo could change, but in this song that is not the case.) At around 100 BPM the tempogram shows the same tempo I concluded as well as the tempo spotify concluded.

Heatmap + dendogram {data-navmenu="First Datas" data-orientation=rows}
====================================

## Graph {data-height="700"}

### Heatmap
```{r}
saan_playlist_mix <-
  get_playlist_audio_features("", "3nvA3PkdTXtslrzxlWF3MA") |>
  add_audio_analysis() |>
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))

halloween_juice <-
  recipe(
    track.name ~
      ##danceability +
      energy +
      loudness +
      speechiness +
      ##acousticness +
      instrumentalness +
      ##liveness +
      valence +
      tempo +
      ##duration +
      #C + `C#|Db` + D + `D#|Eb` +
      #E + `F` + `F#|Gb` + G +
      #`G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 #+
      #c07 + c08 + c09 + c10 + c11 + c12#,
      ,
    data = saan_playlist_mix
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |> 
  # step_range(all_predictors()) |> 
  prep(saan_playlist_mix |> mutate(track.name = str_trunc(track.name, 20))) |>
  juice() |>
  column_to_rownames("track.name")

halloween_dist <- dist(halloween_juice, method = "euclidean")

#halloween_dist |> 
#  hclust(method = "complete") |> # Try single, average, and complete.
#  dendro_data() |>
#  ggdendrogram()

heatmaply(
  halloween_juice,
  hclustfun = hclust,
  hclust_method = "average",  # Change for single, average, or complete linkage.
  dist_method = "euclidean"
)
```

### Dendogram
```{r}
halloween_dist |> 
  hclust(method = "complete") |> # Try single, average, and complete.
  dendro_data() |>
  ggdendrogram()
```

## Text {data-height="300"}

### Look at that!
The dendogram speaks! The original question I had was if the SAAN songs aligned with the songs from the 'radio SAAN' playlist. Now that we have a way to see if the songs fall within a classification we can test this :). 
For this visualisation I made a smaller playlist consisting of 6 SAAN songs and 13 other songs from the radio playlist. The six songs are: 
20point50, Perfect Balance, Saanosphere, Words are no Use, Let it Rain, Don't Let Go. 
As we can see in the dendogram (and the heatmap), these songs are not really put into the same categories. More like they are very spread out! I don't want to make assumptions just from this data, but it looks like the music of SAAN is quite similar (for these specific values) to the music of the playlist. 