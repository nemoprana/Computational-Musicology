---
title: "Exploring SAAN"
subtitle: "Comparing the self-image to the raw data"
author: "Nemo Woudenberg"
output: 
  flexdashboard::flex_dashboard:
    theme: flatly
    orientation: columns
    vertical_layout: fill
---

```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(spotifyr)
library(plotly)
library(compmus)
library(tidymodels)
library(ggdendro)
library(heatmaply)
thematic::thematic_rmd()
```




# Introduction

## Empty {data-width="100"}

## Column {data-width="800"}

### Introduction

Here I will be comparing some interesting data. On the one hand we have one side of the data:

-   My dad has 19 total songs on spotify: 11 songs played with the band 'Mad Stream' and 8 more songs with the band 'Sleepers Awake At Night'. These two bands consist of mostly the same musicians and have (in my opinion) a very similar sound.

I want to use the songs of these two bands to have some fun and make an interesting comparison with another part of the data:

-   The band 'Sleepers Awake At Night' has a public playlist on their spotify profile called 'radio SAAN'. The playlist consists of 94 songs and features some SAAN and Mad Stream songs, but most of the music is from different bands. (From here on out I will refer to the repertoire of SAAN and Mad Stream simply by naming SAAN)

The purpose of this playlist is to give the listener some music that is kind of like the music of SAAN, or that maybe has been an inspiration for the band members. My research will therefore try to look at the next question:

Does SAAN sound like they think they do?

By comparing the playlist with their own songs we might get some insight into the kinds of things the members of SAAN listen for in the music they appreciate. For example: we might find that most of the songs in the radio SAAN playlist have a high score on valence. If the songs made by SAAN also tend to have a high valence, we could conclude that they appreciate a good mood!

For this project I made a playlist containing all the SAAN + Mad Stream songs and another playlist where I copied the radio SAAN playlist and removed all the SAAN and Mad Stream songs (called [SAAN](https://open.spotify.com/playlist/2eBYBIoOoB3AIKyGvbVo3R?si=d42c6f630fb54fe1) and [Radio](https://open.spotify.com/playlist/0Hq8raqKuU0Ln6Df5l2uym?si=f4c36a19d757472e) respectively). 

This portfolio is divided into multiple sections:

[First comparisons](#valence---tempo)

Here we will look at a few simple track-level features provided to us by the spotify API, plotted out in the same way for the SAAN songs and the radio playlist. This will be great to look into some easy data and to forge some early conclusions. The different pages are:

- [Valence - tempo]
- [Energy - loudness]
- [Instrumentalness - speechiness]
- [Time signature]

[Zoom in on tracks](#you-and-me)

Here we will look at some specific tracks from the playlists. We use these tracks to visualize some more specific audio features, and to see if there is a trend with the different playlists. The pages are:
  
- [You and me]
- [20point50]
- [Brave Captain]

[Final comparison](#abc)

This is the page where we will really work towards a final answer. We will compare the two playlists as a whole and see if Spotify can find a difference. The pages are:

- [Timbre](#abc)
- [Classification]

[Conclusion]

Here we make a little final conclusion.


## Empty {data-width="100"}





# Valence - tempo {#valence---tempo data-navmenu="First comparisons"}

## Graph {data-width="700"}

### Valence - tempo -- SAAN

```{r}
SAAN <- get_playlist_audio_features("", "2eBYBIoOoB3AIKyGvbVo3R")

gg_temp_val <- ggplot(SAAN, aes(tempo, valence, color = track.name)) +
  geom_point() +
  theme_minimal() +
  labs(x = "Tempo", y = "Valence", color = "Album") +
  theme(legend.position = "none") +
  xlim(60, 180) +
  ylim(0, 1)

ggplotly(gg_temp_val)
```

## Text {data-width="300"}

### SAAN

Go to [Radio](#vt-radio)

Here on the left we compare the valence and tempo of the playlists (use the link above to switch between the two). 

There seems to be no real correlation between valence and tempo. Both playlists show a wide coverage of both variables, with maybe a bit of emphasis on high valence? Either way, we will have to keep exploring!

[To the next page](#energy---loudness)

The SAAN songs do show a gap in valence between 0.3 and 0.5. Also there are no songs with a bpm around 125, which is actually the most comfortable tempo for humans to tap along to! Is this coincidence? I think so... But it is interesting to note.




# VT radio {#vt-radio .hidden data-navmenu="First comparisons"}

## Graph {data-width="700"}

### Valence - tempo -- Radio

```{r}
radio <- get_playlist_audio_features("", "0Hq8raqKuU0Ln6Df5l2uym")

gg_temp_val <- ggplot(radio, aes(tempo, valence, color = track.name)) +
  geom_point() +
  theme_minimal() +
  labs(x = "Tempo", y = "Valence", color = "Album") +
  theme(legend.position = "none") +
  xlim(60, 180) +
  ylim(0, 1)

ggplotly(gg_temp_val)
```

## Text {data-width="300"}

### Radio

Go to [SAAN](#valence---tempo)

Here on the left we compare the valence and tempo of the playlists (use the link above to switch between the two). 

There seems to be no real correlation between valence and tempo. Both playlists show a wide coverage of both variables, with maybe a bit of emphasis on high valence? Either way, we will have to keep exploring!

[To the next page](#energy---loudness)

If you hover over (or zoom in on) bpm = 171 and valence = 0.76 you will find two songs, Brave Captain and So What'Cha Want, that have almost exactly the same values! There are two more places where this happens, see if you can find them! 



# Energy - loudness {#energy---loudness data-navmenu="First comparisons"}

## Graph {data-width="700"}

### Energy - loudness -- SAAN

```{r}
gg_temp_val <- ggplot(SAAN, aes(loudness, energy, color = track.name)) +
  geom_point() +
  theme_minimal() +
  labs(x = "Loudness", y = "Energy", color = "Track") +
  theme(legend.position = "none") +
  xlim(-15, 0) +
  ylim(0, 1)

ggplotly(gg_temp_val)
```

## Text {data-width="300"}

### SAAN

Go to [Radio](#el-radio)

There seems to be a clear correlation between energy and loudness! This is especially visible for the SAAN songs. Also, both playlists seem to consist of mainly high (above 0.5) energy songs. The SAAN songs cap out on about -7.5db of loudness however, where the Radio playlists seems to contain some louder music.



[To the next page](#instrumentalness---speechiness)



# EL radio {#el-radio .hidden data-navmenu="First comparisons"}

## Graph {data-width="700"}

### Energy - loudness -- Radio

```{r}
gg_temp_val <- ggplot(radio, aes(loudness, energy, color = track.name)) +
  geom_point() +
  theme_minimal() +
  labs(x = "Loudness", y = "Energy", color = "Track") +
  theme(legend.position = "none") +
  xlim(-15, 0) +
  ylim(0, 1)

ggplotly(gg_temp_val)
```

## Text {data-width="300"}

### Radio

Go to [SAAN](#energy---loudness)

There seems to be a clear correlation between energy and loudness! This is especially visible for the SAAN songs. Also, both playlists seem to consist of mainly high (above 0.5) energy songs. The SAAN songs cap out on about -7.5db of loudness however, where the Radio playlists seems to contain some louder music.

[To the next page](#instrumentalness---speechiness)



# Instrumentalness - speechiness {#instrumentalness---speechiness data-navmenu="First comparisons"}

## Graph {data-width="700"}

### Instrumentalness - speechiness - SAAN

```{r}
gg_temp_val <- ggplot(SAAN, aes(instrumentalness, speechiness, color = track.name)) +
  geom_point() +
  theme_minimal() +
  labs(x = "Instrumentalness", y = "Speechiness", color = "Track") +
  theme(legend.position = "none") +
  xlim(0, 1) +
  ylim(0, 1)

ggplotly(gg_temp_val)
```

## Text {data-width="300"}

### SAAN

Go to [Radio](#is-radio)

The main conclusion here: its music with lyrics! Low instrumentalness basically means that there is voice on the recording; low speechiness means it is most likely music with instruments. And almost all songs have both :) The radio playlist does seem to have more instrumental tracks. 

[To the next page](#time-signature)

The song Just Ask Larry (highest instrumentalness) has only talk-like singing and is an exception in that aspect to the other songs.




# IS radio {#is-radio .hidden data-navmenu="First comparisons"}

## Graph {data-width="700"}

### Instrumentalness - speechiness - Radio

```{r}
gg_temp_val <- ggplot(radio, aes(instrumentalness, speechiness, color = track.name)) +
  geom_point() +
  theme_minimal() +
  labs(x = "Instrumentalness", y = "Speechiness", color = "Track") +
  theme(legend.position = "none") +
  xlim(0, 1) +
  ylim(0, 1)

ggplotly(gg_temp_val)
```

## Text {data-width="300"}

### Radio

Go to [SAAN](#instrumentalness---speechiness)

The main conclusion here: its music with lyrics! Low instrumentalness basically means that there is voice on the recording; low speechiness means it is most likely music with instruments. And almost all songs have both :) The radio playlist does seem to have more instrumental tracks. 

[To the next page](#time-signature)


# Time signature {#time-signature data-navmenu="First comparisons"}

## Graph {data-width="700"}

### Time signature - SAAN

```{r}
gg_temp_val <- ggplot(SAAN, aes(time_signature, fill = track.name)) +
  geom_bar() +
  theme_minimal() +
  #labs(x = "Instrumentalness", y = "Speechiness", color = "Track") +
  theme(legend.position = "none") +
  xlim(2, 5) 
  #ylim(0, 1)

ggplotly(gg_temp_val)
```

## Text {data-width="300"}

### SAAN

Go to [Radio](#ts-radio)

This might seem like a bit of a useless page, but it actually tells us a bit! Namely: all the songs have a time signature of 4/4. While most western music is music in 4/4, there are genres and styles where different time signatures are common (jazz and classical for example). By showing that all songs are 4/4 we have found another similarity!

From here on out we will zoom in on some specific tracks.

[To the next page](#you-and-me)

# TS radio {#ts-radio .hidden data-navmenu="First comparisons"}

## Graph {data-width="700"}

### Time signature - Radio

```{r}
gg_temp_val <- ggplot(radio, aes(time_signature, fill = track.name)) +
  geom_bar() +
  theme_minimal() +
  #labs(x = "Instrumentalness", y = "Speechiness", color = "Track") +
  theme(legend.position = "none") +
  xlim(2, 5) 
#ylim(0, 1)

ggplotly(gg_temp_val)
```

## Text {data-width="300"}

### Radio

Go to [SAAN](#time-signature)

This might seem like a bit of a useless page, but it actually tells us a bit! Namely: all the songs have a time signature of 4/4. While most western music is music in 4/4, there are genres and styles where different time signatures are common (jazz and classical for example). By showing that all songs are 4/4 we have found another similarity!

From here on out we will zoom in on some specific tracks.

[To the next page](#you-and-me)

# You and me {data-navmenu="Zoom in on tracks" data-orientation="rows"}

## Text {data-height="300"}

### Just You and Me


Down here are two features of the Mad Stream song 'Just You and Me'. In [First comparisons] this was the song with the lowest valence and a high tempo compared to the other low-valence songs (on the bottom-right).

On the left we have timbre: noticeable here is that timbre c05 shows us where the song contains singing. Some parts are instrumental only, and there c05 is lower. Apparently the singers voice (kind of) occupies the space of c05.

On the right there are chroma features: this shows us how much of a specific note is heard on a specific time. We see one part of the song where the notes Bb and Eb re most dominant, followed up by a part where Gb and Db are dominant. After these parts the clear colors fade and the spectrum is kind of smeared. This might be because of all the percussion in this part. 

[Next page](#tpf)

## Graph {data-height="700"}

### Timbre

```{r}
yandm <-
  get_tidy_audio_analysis("1e7FgxUJ8WAdbVMrpaFpsE") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "max", norm = "chebyshev"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "max", norm = "chebyshev"              # Change summary & norm.
      )
  )

yandm|>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()
```

### Pitch

```{r}
you_and_me <-
  get_tidy_audio_analysis("1e7FgxUJ8WAdbVMrpaFpsE") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

you_and_me |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

# 20point50 {#tpf data-navmenu="Zoom in on tracks" data-orientation="rows"}

## Text {data-height="300"}

### 20point50 self-similairity and chordogram

Show [Novelty and keygram](#tpff)

Here we look at the song '20point50'. I specifically chose this song to zoom in to because the b-part of the song is basically just a big F7 chord. 

On the right here we can see just that! This 'chordogram' shows us which chords sound most prominent in the different sections. If you click on the link above you can see the keygram, which shows us kind of the same developments but with less emphasis on chords and more on structure. 

On the left we see (if we're stil on the keygram page) a novelty function. This functions shows us how much new information is given at a specific time and is used to determine the tempo of the song. The first ~10 seconds of this song don't really have a tempo and are filled with 'electric secrets' (their words, not mine!). The novelty function shows this by giving us a big peak around second 11 where the music and rythm actually start. This is also seen clearly in the self-similarity matrix. 

[Next page](#brave-captain)


## Graph {data-height="700"}

### Self similairity

```{r}
lir <-
  get_tidy_audio_analysis("1yVIMaoLkTQkqhghCiTrNy") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "mean", norm = "manhattan"              # Change summary & norm.
      )
  )

lir |>
  compmus_self_similarity(timbre, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```

### Chordogram

```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

tpf <-
  get_tidy_audio_analysis("1yVIMaoLkTQkqhghCiTrNy") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"
      )
  )

tpf |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

# 20p50 {#tpff data-navmenu="Zoom in on tracks" data-orientation="rows" .hidden}

## Text {data-height="300"}

### Novelty and keygram

Show [Self-similairity and chordogram](#tpf)

Here we look at the song '20point50'. I specifically chose this song to zoom in to because the b-part of the song is basically just a big F7 chord. 

On the right here we can see just that! This 'chordogram' shows us which chords sound most prominent in the different sections. If you click on the link above you can see the keygram, which shows us kind of the same developments but with less emphasis on chords and more on structure. 

On the left we see (if we're stil on the keygram page) a novelty function. This functions shows us how much new information is given at a specific time and is used to determine the tempo of the song. The first ~10 seconds of this song don't really have a tempo and are filled with 'electric secrets' (their words, not mine!). The novelty function shows this by giving us a big peak around second 11 where the music and rythm actually start. This is also seen clearly in the self-similarity matrix. 

[Next page](#brave-captain)



## Graph {data-height="700"}

### Novetly function

```{r}
twpf <-
  get_tidy_audio_analysis("1yVIMaoLkTQkqhghCiTrNy") |>
  select(segments) |>
  unnest(segments)

twpf |>
  mutate(loudness_max_time = start + loudness_max_time) |>
  arrange(loudness_max_time) |>
  mutate(delta_loudness = loudness_max - lag(loudness_max)) |>
  ggplot(aes(x = loudness_max_time, y = pmax(0, delta_loudness))) +
  geom_line() +
  xlim(0, 30) +
  theme_minimal() +
  labs(x = "Time (s)", y = "Novelty")
```

### Keygram

```{r}
tpf |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

# Brave Captain {data-navmenu="Zoom in on tracks" data-orientation="rows"}

## Text {data-height="300"}

### Brave Captain

We take a look at this song to explore a tempogram! I had to pick this song (not from SAAN) because all the SAAN songs are based around a guitar loop.. This means that the tempo will stay exactly the same. 

Brave Captain is from the Radio playlist and shows us clearly here on the left that it has two distinct parts: the part where the bpm is around 350 (which is a tempo-octave, the real tempo is around 175), and the part where we see two vague lines around 200 and 300 bpm (also octaves). 

The chordogram on the right also shows us these different parts.

Now we will look at the final comparisons!
[Next page](#abc)

## Graph {data-height="700"}

### Tempogram

```{r}
tempo <- get_tidy_audio_analysis("3RD6AMDIgly2CO5nrqUnmc")

tempo |>
  tempogram(window_size = 8, hop_size = 3, cyclic = FALSE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()

```

### Chordogram

```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

bc <-
  get_tidy_audio_analysis("3RD6AMDIgly2CO5nrqUnmc") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"
      )
  )

bc |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

# Timbre {#abc data-navmenu="Final comparison"}

## Text {data-width="300"}

### Timbre

Onto the big reveal! Do SAAN and Radio SAAN have similar structures for timbre? The answer (as you could already see on the right) is yes! This is great evidence to support the statement that SAAN sounds very similar to what they have in mind, because timbre is kind of 'the sound' of a band. 

Our final proof will be on the last page.
[Next page](#classification)

## Graph {data-width="700"}

### Timbre comparisons

```{r}
SAANN <-
  get_playlist_audio_features(
    "",
    "2eBYBIoOoB3AIKyGvbVo3R"
  ) |>
  slice(1:30) |>
  add_audio_analysis()
radioo <-
  get_playlist_audio_features(
    "",
    "0Hq8raqKuU0Ln6Df5l2uym"
  ) |>
  slice(1:30) |>
  add_audio_analysis()
comparison <-
  SAANN |>
  mutate(genre = "SAAN") |>
  bind_rows(radioo |> mutate(genre = "Radio"))

comparison |>
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) |>
  select(genre, timbre) |>
  compmus_gather_timbre() |>
  ggplot(aes(x = basis, y = value, fill = genre)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "", fill = "Playlist")
```

# Classification {#classification data-navmenu="Final comparison"}

## Text {data-width="300"}

### Final proof

Show [Heatmap](#cl)

And there is our final proof! To make this dendrogram I made an additional playlist with all the 19 SAAN songs and 19 random songs from the Radio playlist. The test was to see if the dendrogram would make a distinction between SAAN songs and non-SAAN songs. The answer is no! If you look at the [SAAN](https://open.spotify.com/playlist/2eBYBIoOoB3AIKyGvbVo3R?si=d42c6f630fb54fe1) playlist you will see that the names of the songs are not clustered in one area of the dendrogram, but rather form clusters with all the songs! If you take a look at the [heatmap](#cl) you will see the same. The song 'Tour de France' stands out the most, but if you realize that this is a Kraftwerk song it all makes sense. Except for that they compare themselves to Kraftwerk... 

[Next page](#conlusion)

## Graph {data-width="700"}

### Dendrogram

```{r}
saan_playlist_mix <-
  get_playlist_audio_features("", "1PY58G38jccNBg3vx3zZMg") |>
  add_audio_analysis() |>
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))

halloween_juice <-
  recipe(
    track.name ~
      ##danceability +
      energy +
      loudness +
      speechiness +
      ##acousticness +
      instrumentalness +
      ##liveness +
      valence +
      tempo +
      ##duration +
      #C + `C#|Db` + D + `D#|Eb` +
      #E + `F` + `F#|Gb` + G +
      #`G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 #+
    #c07 + c08 + c09 + c10 + c11 + c12#,
    ,
    data = saan_playlist_mix
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |> 
  # step_range(all_predictors()) |> 
  prep(saan_playlist_mix |> mutate(track.name = str_trunc(track.name, 20))) |>
  juice() |>
  column_to_rownames("track.name")

halloween_dist <- dist(halloween_juice, method = "euclidean")

halloween_dist |> 
  hclust(method = "complete") |> # Try single, average, and complete.
  dendro_data() |>
  ggdendrogram()

#heatmaply(
#  halloween_juice,
#  hclustfun = hclust,
#  hclust_method = "average",  # Change for single, average, or complete linkage.
#  dist_method = "euclidean"
#)
```

# cl {#cl .hidden data-navmenu="Final comparison"}

## Text {data-width="300"}

### Final proof

Show [Dendogram](#classification)

And there is our final proof! To make this dendrogram I made an additional playlist with all the 19 SAAN songs and 19 random songs from the Radio playlist. The test was to see if the dendrogram would make a distinction between SAAN songs and non-SAAN songs. The answer is no! If you look at the [SAAN](https://open.spotify.com/playlist/2eBYBIoOoB3AIKyGvbVo3R?si=d42c6f630fb54fe1) playlist you will see that the names of the songs are not clustered in one area of the dendrogram, but rather form clusters with all the songs! If you take a look at the [heatmap](#cl) you will see the same. The song 'Tour de France' stands out the most, but if you realize that this is a Kraftwerk song it all makes sense. Except for that they compare themselves to Kraftwerk... 

[Next page](#conclusion)

## Graph {data-width="700"}

### Heatmap

```{r}
heatmaply(
  halloween_juice,
  hclustfun = hclust,
  hclust_method = "complete",  # Change for single, average, or complete linkage.
  dist_method = "euclidean"
)
```

# Conlusion 

## Empty {data-width="100"}

## Column {data-width="800"}

### Conclusion

There we have it: we have found an answer to our question! SAAN sounds a lot like thay think they do (except for Kraftwerk :) ). We explored some track-level data, went to zoom in on some specific tracks and used classification and timbral analysis to come to this conclusion. 

While there is actually a lot of interesting information we found in this project, I do want to add that this is far from perfect. There are a lot more questions to be asked and a lot more answers to be found. We could for example look at all the tempograms for the SAAN songs to see if they actually stay in perfect time, or we could try classifying on different features to see if there actually is a hidden cluster somewhere. Maybe I will pursue these questions in the future! 

Another shortcoming of this data is that the SAAN playlist has 19 songs in it, while the Radio playlist has 88 songs in it. This is quite a big difference and might have lead to some misinterpretations or  faulty representation. If SAAN releases some more music in the future, we will re-run the calculations to see if it changes anything! 

For now, I hope you enjoyed and maybe learned something new. I know I did!

## Empty {data-width="100"}